{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.15","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"tpu1vmV38","dataSources":[{"sourceId":88046,"databundleVersionId":10229277,"sourceType":"competition"},{"sourceId":104492,"sourceType":"modelInstanceVersion","modelInstanceId":72255,"modelId":76277}],"dockerImageVersionId":30788,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom collections import Counter\nfrom tqdm import tqdm\nimport random, pickle, math, warnings\nimport itertools,  multiprocessing, json\n#warnings.simplefilter('ignore')\nprint(\"CPU Count: \", multiprocessing.cpu_count())\n\np = '/kaggle/input/santa-2024/sample_submission.csv'\ndf = pd.read_csv(p)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-16T03:18:07.608383Z","iopub.execute_input":"2024-12-16T03:18:07.608628Z","iopub.status.idle":"2024-12-16T03:18:09.364257Z","shell.execute_reply.started":"2024-12-16T03:18:07.608593Z","shell.execute_reply":"2024-12-16T03:18:09.363476Z"}},"outputs":[{"name":"stdout","text":"CPU Count:  96\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import transformers, torch, os\nfrom math import exp\n\nDEVICE = torch.device('cpu')\nMODEL_PATH = \"/kaggle/input/gemma-2/transformers/gemma-2-9b/2\"\n\nclass PerplexityCalculator:\n    def __init__(self,):\n        self.tokenizer = transformers.AutoTokenizer.from_pretrained(MODEL_PATH)\n        self.model = transformers.AutoModelForCausalLM.from_pretrained(MODEL_PATH, device_map=\"auto\", torch_dtype=torch.float32,)\n        self.loss_fct = torch.nn.CrossEntropyLoss(reduction='none')\n        self.model.eval()\n\n    #add batch and multiprocessing again for CPU/GPU:}\n    def get_perplexity(self, text: str) -> float:\n        with torch.no_grad():\n            text_with_special = f\"{self.tokenizer.bos_token}{text}{self.tokenizer.eos_token}\"\n            model_inputs = self.tokenizer(text_with_special, return_tensors='pt', add_special_tokens=False,)\n            #model_inputs = {k: v.to(DEVICE) for k, v in model_inputs.items()}\n            logits = self.model(**model_inputs, use_cache=True)['logits']\n            shift_logits = logits[..., :-1, :].contiguous()\n            shift_labels = model_inputs['input_ids'][..., 1:].contiguous()\n            loss = self.loss_fct(\n                shift_logits.view(-1, shift_logits.size(-1)),\n                shift_labels.view(-1))\n            sequence_loss = loss.sum() / len(loss)\n            loss_list = sequence_loss.cpu().item()\n        return exp(loss_list)\n\n# instantiating the model in the strategy scope creates the model on the TPU\n#with tpu_strategy.scope():\n     # define your model normally\nscorer = PerplexityCalculator()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-16T03:18:09.370305Z","iopub.execute_input":"2024-12-16T03:18:09.370595Z","iopub.status.idle":"2024-12-16T03:18:41.284301Z","shell.execute_reply.started":"2024-12-16T03:18:09.370566Z","shell.execute_reply":"2024-12-16T03:18:41.283276Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n/usr/local/lib/python3.10/site-packages/torch_xla/__init__.py:202: UserWarning: `tensorflow` can conflict with `torch-xla`. Prefer `tensorflow-cpu` when using PyTorch/XLA. To silence this warning, `pip uninstall -y tensorflow && pip install tensorflow-cpu`. If you are in a notebook environment such as Colab or Kaggle, restart your notebook runtime afterwards.\n  warnings.warn(\nLoading checkpoint shards: 100%|██████████| 8/8 [00:02<00:00,  2.85it/s]\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"t = \"\"\"reindeer mistletoe elf gingerbread family advent scrooge chimney fireplace ornament\nreindeer walk gingerbread bake the night and sleep scrooge drive chimney jump elf laugh mistletoe give family advent fireplace ornament\nmagi yuletide cheer grinch carol holiday holly jingle naughty nice nutcracker polar beard ornament stocking chimney sleigh workshop gifts decorations\nsleigh of the magi yuletide cheer is unwrap gifts and eat cheer holiday decorations holly jingle relax carol sing chimney visit grinch naughty nice polar beard workshop nutcracker ornament stocking\nfrom and as have in not it of that the to we with you bow angel believe candle candy card chocolate cookie doll dream eggnog fireplace fruitcake game greeting hohoho hope joy kaggle merry milk night paper peace peppermint poinsettia puzzle season snowglobe star toy wreath wish workshop wonder wrapping\nfrom and and as and have the in is it of of not that the to we with you advent card the angel bake beard believe bow candy candle carol cheer cheer chocolate chimney cookie decorations doll dream drive eat eggnog family fireplace fireplace chimney fruitcake game give gifts gingerbread greeting grinch holiday holly hohoho hope jingle jump joy kaggle laugh magi merry milk mistletoe naughty nice night night elf nutcracker ornament ornament wrapping paper peace peppermint polar poinsettia puzzle reindeer relax scrooge season sing sleigh sleep snowglobe star stocking toy unwrap visit walk wish wonder workshop workshop wreath yuletide\"\"\"\n\ndf['text'] = t.split('\\n')\ndf['score'] = df['text'].map(lambda x: scorer.get_perplexity(x))\ndf.to_csv(\"submission.csv\", index=False)\nprint(np.mean(df['score']))\ndf['score']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-16T03:18:41.286180Z","iopub.execute_input":"2024-12-16T03:18:41.286847Z","iopub.status.idle":"2024-12-16T03:19:35.185695Z","shell.execute_reply.started":"2024-12-16T03:18:41.286812Z","shell.execute_reply":"2024-12-16T03:19:35.184952Z"}},"outputs":[{"name":"stderr","text":"Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n","output_type":"stream"},{"name":"stdout","text":"262.95834068043825\n","output_type":"stream"},{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"0    468.499121\n1    463.726354\n2    303.031473\n3    209.184454\n4     98.260833\n5     35.047809\nName: score, dtype: float64"},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"past = {}\n\ndef part_perm_brutem(st, start=0, end=3, skips=1, tol=1.0, smax=300):\n    global past\n    bestt = st\n    best = scorer.get_perplexity(st)\n    st = st.split(' ')\n    part = st[start:end]\n    if start>0:\n        st1 =  ' '.join(st[:start]) + ' '\n    else:\n        st1 = ''\n    if end<len(st): \n        st2 =  ' ' + ' '.join(st[end:])\n    else: \n        st2 = ''\n    p = list(itertools.permutations(part))\n    for i in range(0, len(p), skips): #removed tqdm\n        t =  st1 + ' '.join(list(p[i])) + st2\n        if t in past:\n            s = past[t]\n        else:\n            s =  scorer.get_perplexity(t)\n        #if s <= best * tol and t not in past and s<smax:\n        if s < best and t not in past:\n            print(\"New Score: \", s, t)\n            best = s\n            bestt = t\n        if t not in past:\n            past[t] = s\n    return bestt","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-16T03:19:35.186777Z","iopub.execute_input":"2024-12-16T03:19:35.187052Z","iopub.status.idle":"2024-12-16T03:19:35.194997Z","shell.execute_reply.started":"2024-12-16T03:19:35.187024Z","shell.execute_reply":"2024-12-16T03:19:35.194302Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"i = 0\nbestt = df['text'][i]\nl = len(df['text'][i].split(' '))\nfor p in range(2, 3):\n    for start in range(0,l-p+1):\n        print(\"START: \", start)\n        bestt = part_perm_brutem(bestt, start, start+p, 1)\n        df.to_csv(\"submission.csv\", index=False)\n\ndf['score'] = df['text'].map(lambda x: scorer.get_perplexity(x))\ndf.to_csv(\"submission.csv\", index=False)\nprint(np.mean(df['score']))\ndf['score']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-16T03:19:35.195911Z","iopub.execute_input":"2024-12-16T03:19:35.196189Z","iopub.status.idle":"2024-12-16T03:20:07.545888Z","shell.execute_reply.started":"2024-12-16T03:19:35.196161Z","shell.execute_reply":"2024-12-16T03:20:07.544978Z"}},"outputs":[{"name":"stdout","text":"START:  0\nSTART:  1\nSTART:  2\nSTART:  3\nSTART:  4\nSTART:  5\nSTART:  6\nSTART:  7\nSTART:  8\n262.95834068043825\n","output_type":"stream"},{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"0    468.499121\n1    463.726354\n2    303.031473\n3    209.184454\n4     98.260833\n5     35.047809\nName: score, dtype: float64"},"metadata":{}}],"execution_count":6}]}